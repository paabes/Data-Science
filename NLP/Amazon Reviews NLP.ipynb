{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review:\n",
    "    def __init__(self, category, text):\n",
    "        self.category = category\n",
    "        self.text = text    \n",
    "        \n",
    "class ReviewContainer:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews = reviews\n",
    "    \n",
    "    def get_text(self):\n",
    "        return [x.text for x in self.reviews]\n",
    "    \n",
    "    def get_y(self):\n",
    "        return [x.category for x in self.reviews]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prep Training/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = []\n",
    "all_categories = []\n",
    "for file in os.listdir('./data/training'):\n",
    "    category = file.strip('train_').split('.')[0]\n",
    "    all_categories.append(category)\n",
    "    with open(f'./data/training/{file}') as f:\n",
    "        for line in f:\n",
    "            review_json = json.loads(line)\n",
    "            review = Review(category, review_json['reviewText'])\n",
    "            train_reviews.append(review)\n",
    "\n",
    "train_container = ReviewContainer(train_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reviews = []\n",
    "for file in os.listdir('./data/test'):\n",
    "    category = file.strip('test_').split('.')[0]\n",
    "    with open(f'./data/test/{file}') as f:\n",
    "        for line in f:\n",
    "            review_json = json.loads(line)\n",
    "            review = Review(category, review_json['reviewText'])\n",
    "            test_reviews.append(review)\n",
    "            \n",
    "test_container = ReviewContainer(test_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "corpus = train_container.get_text()\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "train_x = vectorizer.fit_transform(corpus) # training text converted to vectors\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x, train_container.get_y())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Performance (Bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to convert test text to vector form\n",
    "test_corpus = test_container.get_text()\n",
    "test_x = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6522222222222223\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.5484222  0.46201074 0.71557971 0.46501129 0.70614035 0.79538905\n",
      " 0.66816143 0.71020408 0.82866044]\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy:\", clf.score(test_x, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram approach to Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regular approach to bag of words model is unigram, meaning that the model assesses individual words, however, When trying to infer sentiment from a sentence, context matters, \"great\" is quite different from \"not great\", so now let's try to use combination of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_container.get_text()\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1,2))\n",
    "train_x = vectorizer.fit_transform(corpus) # training text converted to vector\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x, train_container.get_y())\n",
    "\n",
    "test_corpus = test_container.get_text()\n",
    "test_x = vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6097777777777778\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.46918919 0.41161401 0.69604317 0.37086093 0.6416309  0.80545455\n",
      " 0.61470911 0.66188525 0.7983454 ]\n"
     ]
    }
   ],
   "source": [
    "# check if performance has improved\n",
    "\n",
    "print(\"Overall Accuracy:\", clf.score(test_x, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Vectors Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md') # English pipeline optimized for cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_raw = train_container.get_text()\n",
    "\n",
    "docs_train = [nlp(text) for text in train_x_raw] # both text and vectors\n",
    "\n",
    "train_x_word_vectors = [x.vector for x in docs_train] #only vectors\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x_word_vectors, train_container.get_y())\n",
    "\n",
    "test_x_raw = test_container.get_text()\n",
    "docs_test = [nlp(text) for text in test_x_raw] # both text and vectors\n",
    "test_x_word_vectors = [x.vector for x in docs_test] #only vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7131111111111111\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.65784114 0.54347826 0.76080692 0.55747711 0.78958785 0.74114441\n",
      " 0.75339367 0.77603143 0.86217617]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating performance\n",
    "print(\"Overall Accuracy:\", clf.score(test_x_word_vectors, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x_word_vectors)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word vectors are quite powerful tool, as it tries to infer relationship between words, thus performing better with previously unencountered combination of words: this approach encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning. The result is obvious, accuracy of our svc model went up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['do', 'all', 'the', 'project', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'do all the project .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# test it\n",
    "\n",
    "phrase = \"doing all the projects.\"\n",
    "tokenized_words = word_tokenize(phrase)\n",
    "\n",
    "stemmed_words = [stemmer.stem(x) for x in tokenized_words]\n",
    "print(stemmed_words)\n",
    "' '.join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The drawback of this approach is that it uses an algorithm to stem the words, not a real-world dictionary-based approach which makes it vulnerable to lexical ambiguities and punctual mistakes, for instance counting commas and periods as stand-alone words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just trying  to    remove   special  characters   what  did it   work       \n",
      "['just', 'tri', 'to', 'remov', 'special', 'charact', 'what', 'did', 'it', 'work']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'just tri to remov special charact what did it work'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since stemming does not ignore special characters, we have to clean the review data first\n",
    "# experimenting with regexes\n",
    "\n",
    "ph = \"just trying, to/ ^ remove*) special. characters & what? did it!! work?% >/$ \"\n",
    "\n",
    "import re\n",
    "s = re.sub(r\"[^a-zA-Z0-9]\",\" \",ph)\n",
    "print(s)\n",
    "\n",
    "tokenized_words = word_tokenize(s)\n",
    "stemmed_words = [stemmer.stem(x) for x in tokenized_words]\n",
    "print(stemmed_words)\n",
    "' '.join(stemmed_words)\n",
    "\n",
    "# yup, this looks just right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love this song \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'so far  so good '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recreating word vector model with stemming\n",
    "\n",
    "# step 1 - cleaning special characters\n",
    "# step 2 - stemming train and test x data\n",
    "# step 3 - re-implementing word-vector SVC model\n",
    "\n",
    "train_x_raw = train_container.get_text()\n",
    "test_x_raw = test_container.get_text()\n",
    "\n",
    "train_x_raw2 = [re.sub(r\"[^a-zA-Z0-9]\",\" \", x) for x in train_x_raw]\n",
    "test_x_raw2 = [re.sub(r\"[^a-zA-Z0-9]\",\" \", x) for x in test_x_raw]\n",
    "\n",
    "print(test_x_raw2[0])\n",
    "train_x_raw2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [word_tokenize(x) for x in train_x_raw2]\n",
    "tokenized_test = [word_tokenize(x) for x in test_x_raw2]\n",
    "\n",
    "#train_x_stemmed = [stemmer.stem(x) for x in tokenized_train]\n",
    "#test_x_stemmed = [stemmer.stem(x) for x in tokenized_test]\n",
    "\n",
    "train_x_stemmed = []\n",
    "test_x_stemmed = []\n",
    "\n",
    "for review in tokenized_train:\n",
    "    train_x_stemmed.append(' '.join([stemmer.stem(x) for x in review]))\n",
    "    \n",
    "for review in tokenized_test:\n",
    "    test_x_stemmed.append(' '.join([stemmer.stem(x) for x in review]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = [nlp(text) for text in train_x_stemmed] # both text and vectors\n",
    "\n",
    "train_x_word_vectors = [x.vector for x in docs_train] #only vectors\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x_word_vectors, train_container.get_y())\n",
    "\n",
    "docs_test = [nlp(text) for text in test_x_stemmed] # both text and vectors\n",
    "test_x_word_vectors = [x.vector for x in docs_test] #only vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6795555555555556\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.62741313 0.52173913 0.73904762 0.50362694 0.73005464 0.71103008\n",
      " 0.70046083 0.74658869 0.85031185]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating performance\n",
    "print(\"Overall Accuracy:\", clf.score(test_x_word_vectors, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x_word_vectors)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance does not seem to have improved. Let's now try lemmatizing instead\n",
    "\n",
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_lemmatized = []\n",
    "test_x_lemmatized = []\n",
    "\n",
    "for review in tokenized_train:\n",
    "    train_x_lemmatized.append(' '.join([lemmatizer.lemmatize(x, pos='v') for x in review]))\n",
    "    \n",
    "for review in tokenized_test:\n",
    "    test_x_lemmatized.append(' '.join([lemmatizer.lemmatize(x, pos='v') for x in review]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train = [nlp(text) for text in train_x_lemmatized] # both text and vectors\n",
    "\n",
    "train_x_word_vectors = [x.vector for x in docs_train] #only vectors\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x_word_vectors, train_container.get_y())\n",
    "\n",
    "docs_test = [nlp(text) for text in test_x_lemmatized] # both text and vectors\n",
    "test_x_word_vectors = [x.vector for x in docs_test] #only vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.7128888888888889\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.66260163 0.54721977 0.77312561 0.56352459 0.77105832 0.74074074\n",
      " 0.73496659 0.76424361 0.88517745]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating performance\n",
    "print(\"Overall Accuracy:\", clf.score(test_x_word_vectors, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x_word_vectors)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding + Regex filtering + lemmatization + stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x_raw = train_container.get_text()\n",
    "test_x_raw = test_container.get_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so', 'far', 'so', 'good']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_raw2 = [re.sub(r\"[^a-zA-Z0-9]\",\" \", x) for x in train_x_raw]\n",
    "test_x_raw2 = [re.sub(r\"[^a-zA-Z0-9]\",\" \", x) for x in test_x_raw]\n",
    "\n",
    "tokenized_train = [word_tokenize(x) for x in train_x_raw2]\n",
    "tokenized_test = [word_tokenize(x) for x in test_x_raw2]\n",
    "\n",
    "tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love song'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "stripped_train = []\n",
    "stripped_word = []\n",
    "\n",
    "for lst in tokenized_train:\n",
    "    for word in lst:\n",
    "        if word not in stop_words:\n",
    "            stripped_word.append(word)\n",
    "    stripped_train.append(' '.join(stripped_word))\n",
    "    stripped_word = [] \n",
    "    \n",
    "\n",
    "stripped_test = []\n",
    "stripped_word = []\n",
    "\n",
    "for lst in tokenized_test:\n",
    "    for word in lst:\n",
    "        if word not in stop_words:\n",
    "            stripped_word.append(word)\n",
    "    stripped_test.append(' '.join(stripped_word))\n",
    "    stripped_word = []\n",
    "    \n",
    "stripped_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love song'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_lemmatized = []\n",
    "test_x_lemmatized = []\n",
    "\n",
    "for review in stripped_train:\n",
    "    train_x_lemmatized.append(''.join([lemmatizer.lemmatize(x, pos='v') for x in review]))\n",
    "    \n",
    "for review in stripped_test:\n",
    "    test_x_lemmatized.append(''.join([lemmatizer.lemmatize(x, pos='v') for x in review]))\n",
    "    \n",
    "test_x_lemmatized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs_train = [nlp(text) for text in train_x_lemmatized]\n",
    "train_x_word_vectors = [x.vector for x in docs_train] \n",
    "\n",
    "docs_test = [nlp(text) for text in test_x_lemmatized]\n",
    "test_x_word_vectors = [x.vector for x in docs_test]\n",
    "\n",
    "clf = svm.SVC(kernel='linear')\n",
    "clf.fit(train_x_word_vectors, train_container.get_y())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.716\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.66188525 0.5443787  0.76453765 0.5738576  0.77705628 0.74861368\n",
      " 0.75968992 0.77675841 0.86992716]\n"
     ]
    }
   ],
   "source": [
    "# Evaluating performance\n",
    "print(\"Overall Accuracy:\", clf.score(test_x_word_vectors, test_container.get_y()))\n",
    "\n",
    "y_pred = clf.predict(test_x_word_vectors)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Random Forest instead of SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cls = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(train_x_word_vectors, train_container.get_y())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.6908888888888889\n",
      "f1 scores by category\n",
      "['Electronics', 'Automotive', 'Digital_Music', 'Patio_Lawn_Garden', 'Grocery', 'Beauty', 'Pet_Supplies', 'Clothing', 'Books']\n",
      "[0.61132075 0.51171875 0.77145612 0.4420218  0.7742616  0.86407767\n",
      " 0.66592428 0.73209028 0.85333333]\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy:\", cls.score(test_x_word_vectors, test_container.get_y()))\n",
    "\n",
    "y_pred = cls.predict(test_x_word_vectors)\n",
    "\n",
    "print(\"f1 scores by category\")\n",
    "print(all_categories)\n",
    "print(f1_score(test_container.get_y(), y_pred, average=None, labels=all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 1306, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\", line 444, in _check_solver\n",
      "    \"got %s penalty.\" % (solver, penalty))\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:921: UserWarning: One or more of the test scores are non-finite: [0.664   nan   nan]\n",
      "  category=UserWarning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>best_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.639259</td>\n",
       "      <td>{'max_features': 'sqrt'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svc</td>\n",
       "      <td>0.672296</td>\n",
       "      <td>{'kernel': 'linear'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.424148</td>\n",
       "      <td>{'splitter': 'best'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>{'penalty': 'none'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  best_score               best_params\n",
       "0        random_forest    0.639259  {'max_features': 'sqrt'}\n",
       "1                  svc    0.672296      {'kernel': 'linear'}\n",
       "2        decision_tree    0.424148      {'splitter': 'best'}\n",
       "3  logistic_regression    0.664000       {'penalty': 'none'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_best_model_using_gridsearchcv(X,y):\n",
    "    algos = {\n",
    "        'random_forest' : {\n",
    "            'model': RandomForestClassifier(),\n",
    "            'params': {\n",
    "                'max_features': ['auto', 'sqrt']\n",
    "            }\n",
    "        },\n",
    "        'svc': {\n",
    "            'model': svm.SVC(),\n",
    "            'params': {\n",
    "                'kernel':['linear']\n",
    "            }\n",
    "        },\n",
    "        'decision_tree': {\n",
    "            'model': DecisionTreeClassifier(),\n",
    "            'params': {\n",
    "                'splitter': ['best','random']\n",
    "            }\n",
    "        },\n",
    "        'logistic_regression': {\n",
    "            'model': LogisticRegression(),\n",
    "            'params': {\n",
    "                'penalty':['none', 'l1', 'elasticnet']\n",
    "            }\n",
    "        \n",
    "    }\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "    for algo_name, config in algos.items():\n",
    "        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n",
    "        gs.fit(X,y)\n",
    "        scores.append({\n",
    "            'model': algo_name,\n",
    "            'best_score': gs.best_score_,\n",
    "            'best_params': gs.best_params_\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n",
    "\n",
    "find_best_model_using_gridsearchcv(test_x_word_vectors,test_container.get_y())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seems like SVC is, indeed the best conventional choice for this task, however it is still not accurate enough. in the bert_model file I try to solve same classification problem, but this time with spacy bert transformer ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
