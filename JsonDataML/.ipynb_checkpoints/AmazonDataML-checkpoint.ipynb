{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='./data/books_small.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"reviewerID\": \"A1E5ZR1Z4OQJG\", \"asin\": \"1495329321\", \"reviewerName\": \"Pure Jonel \\\"Pure Jonel\\\"\", \"helpful\": [0, 0], \"reviewText\": \"Da Silva takes the divine by storm with this unique new novel.  She develops a world unlike any others while keeping it firmly in the real world.  This is a very well written and entertaining novel.  I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story.  I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel.  Da Silva creates a cast of high school students who actually act like high school students.  I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts.  It was very refreshing and added to the realism and impact of the novel.  The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic.  I can&#8217;t wait to read more and to find out what happens next in the series.  I&#8217;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.\", \"overall\": 4.0, \"summary\": \"An amazing first novel\", \"unixReviewTime\": 1396137600, \"reviewTime\": \"03 30, 2014\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(file_path) as f:\n",
    "    for entry in f:\n",
    "        print(entry)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da Silva takes the divine by storm with this unique new novel.  She develops a world unlike any others while keeping it firmly in the real world.  This is a very well written and entertaining novel.  I was quite impressed and intrigued by the way that this solid storyline was developed, bringing the readers right into the world of the story.  I was engaged throughout and definitely enjoyed my time spent reading it.I loved the character development in this novel.  Da Silva creates a cast of high school students who actually act like high school students.  I really appreciated the fact that none of them were thrown into situations far beyond their years, nor did they deal with events as if they had decades of life experience under their belts.  It was very refreshing and added to the realism and impact of the novel.  The friendships between the characters in this novel were also truly touching.Overall, this novel was fantastic.  I can&#8217;t wait to read more and to find out what happens next in the series.  I&#8217;d definitely recommend this debut novel by Da Silva to those who want a little YA fun with a completely unique & shocking storyline.Please note that I received a complimentary copy of this work in exchange for an honest review.\n"
     ]
    }
   ],
   "source": [
    "# let's now try to access individual features by converting json to python dictionary\n",
    "      \n",
    "with open(file_path) as f:\n",
    "    for entry in f:\n",
    "        review=json.loads(entry)\n",
    "        print(review['reviewText'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's now convert entire json dataset into list of review objects that contain review text and rating score out of 5\n",
    "\n",
    "to simplyfy calling, let's define a reeview class, so that to retrieve score for instance, we can do so by reviews.score and not reviews[x][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplyfy feedback assigning by defining enum-like class\n",
    "class Feedback:\n",
    "    NEGATIVE=\"NEGATIVE\"\n",
    "    POSITIVE=\"POSITIVE\"\n",
    "    NEUTRAL=\"NEUTRAL\"\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, score):\n",
    "        self.text=text\n",
    "        self.score=score\n",
    "        self.feedback=self.get_feedback()\n",
    "        \n",
    "    def get_feedback(self):\n",
    "        if self.score <=2:\n",
    "            return Feedback.NEGATIVE\n",
    "        elif self.score == 3:\n",
    "            return Feedback.NEUTRAL\n",
    "        else: return Feedback.POSITIVE\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I had not previously read this author but heard a review of this book on the radio and knew I had to read it. Even we baby-boomers have huge gaps in our knowledge of World War II - Unbroken fills in gaps and brings it home to the reader. I recommend it to readers of all ages. Our fathers and grandfathers who fought this war didn't think they were doing anything but their job.  Those, like Louie, who became POWs, found courage they never thought they'd have. As I read this, I wondered what I would have done in his place. I read this book in two sittings.\""
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=[]\n",
    "with open(file_path) as f:\n",
    "    for entry in f:\n",
    "        review=json.loads(entry)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))\n",
    "        \n",
    "# check if it worked\n",
    "reviews[18]\n",
    "reviews[18].score\n",
    "reviews[18].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Preprocessing of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test= train_test_split(reviews, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x=[x.text for x in training]\n",
    "train_y=[x.feedback for x in training]\n",
    "\n",
    "test_x=[x.text for x in test]\n",
    "test_y=[x.feedback for x in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorizing  categorical features with bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you won't want to put it down. very easy to get interested in from the beginning of the book. I thought it was good enough for a five star rating.\n",
      "  (0, 7927)\t1\n",
      "  (0, 7850)\t1\n",
      "  (0, 7680)\t1\n",
      "  (0, 7246)\t2\n",
      "  (0, 5633)\t1\n",
      "  (0, 3838)\t2\n",
      "  (0, 2165)\t1\n",
      "  (0, 7605)\t1\n",
      "  (0, 2275)\t1\n",
      "  (0, 3044)\t1\n",
      "  (0, 3747)\t1\n",
      "  (0, 3627)\t1\n",
      "  (0, 2939)\t1\n",
      "  (0, 7131)\t2\n",
      "  (0, 745)\t1\n",
      "  (0, 4949)\t1\n",
      "  (0, 897)\t1\n",
      "  (0, 7179)\t1\n",
      "  (0, 7700)\t1\n",
      "  (0, 3101)\t1\n",
      "  (0, 2416)\t1\n",
      "  (0, 2855)\t1\n",
      "  (0, 2779)\t1\n",
      "  (0, 6717)\t1\n",
      "  (0, 5703)\t1\n"
     ]
    }
   ],
   "source": [
    "vectorizer=CountVectorizer()\n",
    "train_xv=vectorizer.fit_transform(train_x)\n",
    "test_xv=vectorizer.transform(test_x)\n",
    "\n",
    "print(train_x[0])\n",
    "print(train_xv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_cls=svm.SVC(kernel='linear')\n",
    "svm_cls.fit(train_xv, train_y)\n",
    "\n",
    "test_x[0]\n",
    "svm_cls.predict(train_xv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decTree_cls=DecisionTreeClassifier()\n",
    "decTree_cls.fit(train_xv, train_y)\n",
    "\n",
    "decTree_cls.predict(train_xv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_cls=svm.SVC(kernel='rbf')\n",
    "svm_rbf_cls.fit(train_xv, train_y)\n",
    "\n",
    "test_x[0]\n",
    "svm_rbf_cls.predict(train_xv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['POSITIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cls=LogisticRegression()\n",
    "log_cls.fit(train_xv, train_y)\n",
    "\n",
    "log_cls.predict(train_xv[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828\n",
      "0.836\n",
      "0.86\n",
      "0.752\n"
     ]
    }
   ],
   "source": [
    "# Mean Accuracy\n",
    "\n",
    "print(svm_cls.score(test_xv, test_y))\n",
    "print(log_cls.score(test_xv, test_y))\n",
    "print(svm_rbf_cls.score(test_xv, test_y))\n",
    "print(decTree_cls.score(test_xv, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9086758 , 0.3       , 0.23809524])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f1 scores\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(test_y,svm_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.91928251, 0.125     , 0.15789474])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,log_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92473118, 0.        , 0.        ])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,svm_rbf_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85849057, 0.15384615, 0.16      ])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(test_y,decTree_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Metrics show that all the models seem to be biased towards feedback=positive and are remarkably innacurate with feedback=negative and feedback=neutral predictions. It is almost certainly a problem that is indroduced by dataset rather that the model configurations so let's check it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's first check what is the ration of positive/negative feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 51\n",
      "positive: 620\n",
      "neutral: 79\n"
     ]
    }
   ],
   "source": [
    "print(\"negative: \" + str(train_y.count(Feedback.NEGATIVE)))\n",
    "print(\"positive: \" + str(train_y.count(Feedback.POSITIVE)))\n",
    "print(\"neutral: \" + str(train_y.count(Feedback.NEUTRAL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as suspected, training dataset itself is heavily biased. one solution would be to use a bigger dataset, in addition to this, let's create a class that evens out positive-negative feedbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Renormaliser:\n",
    "    def __init__(self, reviews):\n",
    "        self.reviews=reviews\n",
    "        \n",
    "    def renormalise(self):\n",
    "        negatives=list(filter(lambda x: x.feedback==Feedback.NEGATIVE, self.reviews))\n",
    "        positives=list(filter(lambda x: x.feedback==Feedback.POSITIVE, self.reviews))\n",
    "        positive_reduced=positives[:len(negatives)]\n",
    "        self.reviews=negatives+positive_reduced\n",
    "        #rearrange to introduce randomness\n",
    "        random.shuffle(self.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_file_path='./data/books_small_10000.json'\n",
    "   \n",
    "reviews=[]\n",
    "with open(large_file_path) as f:\n",
    "    for entry in f:\n",
    "        review=json.loads(entry)\n",
    "        reviews.append(Review(review['reviewText'], review['overall']))\n",
    "        \n",
    "# go through same steps\n",
    "\n",
    "training, test= train_test_split(reviews, test_size=0.25, random_state=42)\n",
    "\n",
    "# renormilse dataset such that it is not biased towards positive feedback\n",
    "\n",
    "renormalizer=Renormaliser(training)\n",
    "renormalizer.renormalise()\n",
    "\n",
    "train_x=[x.text for x in renormalizer.reviews]\n",
    "train_y=[x.feedback for x in renormalizer.reviews]\n",
    "\n",
    "test_renorm=Renormaliser(test)\n",
    "test_renorm.renormalise()\n",
    "\n",
    "test_x=[x.text for x in test_renorm.reviews]\n",
    "test_y=[x.feedback for x in test_renorm.reviews]\n",
    "\n",
    "#different models\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "train_xv=vectorizer.fit_transform(train_x)\n",
    "test_xv=vectorizer.transform(test_x)\n",
    "\n",
    "svm_cls=svm.SVC(kernel='linear')\n",
    "svm_cls.fit(train_xv, train_y)\n",
    "\n",
    "decTree_cls=DecisionTreeClassifier()\n",
    "decTree_cls.fit(train_xv, train_y)\n",
    "\n",
    "svm_rbf_cls=svm.SVC(kernel='rbf')\n",
    "svm_rbf_cls.fit(train_xv, train_y)\n",
    "\n",
    "log_cls=LogisticRegression()\n",
    "log_cls.fit(train_xv, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "972"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(renormalizer.reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7879746835443038\n",
      "0.819620253164557\n",
      "0.7658227848101266\n",
      "0.6392405063291139\n"
     ]
    }
   ],
   "source": [
    "print(svm_cls.score(test_xv, test_y))\n",
    "print(log_cls.score(test_xv, test_y))\n",
    "print(svm_rbf_cls.score(test_xv, test_y))\n",
    "print(decTree_cls.score(test_xv, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666667 0.60689655 0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76875    0.76282051 0.        ]\n",
      "[0.82352941 0.81553398 0.        ]\n",
      "[0.79256966 0.78317152 0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(test_y,decTree_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL]))\n",
    "print(f1_score(test_y,svm_rbf_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL]))\n",
    "print(f1_score(test_y,log_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL]))\n",
    "print(f1_score(test_y,svm_cls.predict(test_xv), average=None, \\\n",
    "         labels=[Feedback.POSITIVE, Feedback.NEGATIVE, Feedback.NEUTRAL]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative: 486\n",
      "positive: 486\n",
      "neutral: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"negative: \" + str(train_y.count(Feedback.NEGATIVE)))\n",
    "print(\"positive: \" + str(train_y.count(Feedback.POSITIVE)))\n",
    "print(\"neutral: \" + str(train_y.count(Feedback.NEUTRAL)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now test manually how well our models got at predicting whether random text review is positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE', 'POSITIVE', 'NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set=[\"do not reccomend, rather underwhealming\", \"enjoyed it quite a bit\", \"wase of money\"]\n",
    "vector_test=vectorizer.transform(test_set)\n",
    "\n",
    "svm_cls.predict(vector_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM classifier seems to be working rather well on this dataset, let's see if we can increase it's accuracy further, by trying and comparing different parameter options with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
       "                           class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'C': (0.001, 1, 4, 8, 16, 32),\n",
       "                         'kernel': ('linear', 'poly', 'rbf', 'sigmoid')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'kernel': (\"linear\", \"poly\", 'rbf', 'sigmoid'), 'C': (0.001,1,4,8,16,32)}\n",
    "\n",
    "svc = svm.SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=5)\n",
    "clf.fit(train_xv, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9794238683127572\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(train_xv, train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is impressive! let's finally save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store The Selected Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./models/feedback_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the stored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./models/feedback_classifier.pkl', 'rb') as f:\n",
    "    loaded_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too short to be areal story and so don't expect one and over before you even get interested in whats happening\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['NEGATIVE'], dtype='<U8')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_x[0])\n",
    "\n",
    "loaded_clf.predict(test_xv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
